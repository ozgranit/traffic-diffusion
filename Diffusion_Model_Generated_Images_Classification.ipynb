{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9I-uq-Vj4wnO"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import pickle\n",
        "import time\n",
        "import json\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.dataloader import DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r sd_xl"
      ],
      "metadata": {
        "id": "ZHj1LnV49NKx"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1W8JYgjDgTWLFCvchHHZ2P_uhS3VS-eGt\n",
        "!unzip -o sd_xl.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG_280T18JOj",
        "outputId": "1f574e87-6e2b-4aae-ace0-adaacfb0cae4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1W8JYgjDgTWLFCvchHHZ2P_uhS3VS-eGt\n",
            "To: /content/sd_xl.zip\n",
            "\r  0% 0.00/8.98M [00:00<?, ?B/s]\r100% 8.98M/8.98M [00:00<00:00, 290MB/s]\n",
            "Archive:  sd_xl.zip\n",
            "  inflating: sd_xl/road66_sd.png     \n",
            "  inflating: sd_xl/road66_sd_1.png   \n",
            "  inflating: sd_xl/road66_sd_10.png  \n",
            "  inflating: sd_xl/road66_sd_11.png  \n",
            "  inflating: sd_xl/road66_sd_12.png  \n",
            "  inflating: sd_xl/road66_sd_13.png  \n",
            "  inflating: sd_xl/road66_sd_14.png  \n",
            "  inflating: sd_xl/road66_sd_15.png  \n",
            "  inflating: sd_xl/road66_sd_16.png  \n",
            "  inflating: sd_xl/road66_sd_17.png  \n",
            "  inflating: sd_xl/road66_sd_18.png  \n",
            "  inflating: sd_xl/road66_sd_19.png  \n",
            "  inflating: sd_xl/road66_sd_2.png   \n",
            "  inflating: sd_xl/road66_sd_3.png   \n",
            "  inflating: sd_xl/road66_sd_4.png   \n",
            "  inflating: sd_xl/road66_sd_5.png   \n",
            "  inflating: sd_xl/road66_sd_6.png   \n",
            "  inflating: sd_xl/road66_sd_7.png   \n",
            "  inflating: sd_xl/road66_sd_8.png   \n",
            "  inflating: sd_xl/road66_sd_9.png   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQdah3KX3S1T",
        "outputId": "94936db2-d236-4491-bdd6-d3bfe8a8a44b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1C0k77EeZrByBUdv36IxS9PiLUvZXRr24\n",
            "To: /content/model.zip\n",
            "100% 129M/129M [00:00<00:00, 193MB/s]\n",
            "Archive:  model.zip\n",
            "  inflating: model_gtsrb.pth         \n",
            "  inflating: model_lisa.pth          \n",
            "  inflating: adv_model_gtsrb.pth     \n",
            "  inflating: adv_model_lisa.pth      \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Simsmw-H4zKgF5FtJ934bthYulzRlkqz\n",
            "To: /content/diffusion_imgs.zip\n",
            "100% 2.88M/2.88M [00:00<00:00, 234MB/s]\n",
            "Archive:  diffusion_imgs.zip\n",
            "  inflating: getimg_ai_img-qbD8yQW1ct0CQgu6BgPCH.png  \n",
            "  inflating: getimg_ai_img-r9pqFJTHiZBSnBJoZBmoz (1).png  \n",
            "  inflating: getimg_ai_img-SiN26Z5uuPC2iKTYWBmua.png  \n",
            "  inflating: params.json             \n",
            "  inflating: stop_sign.jpg           \n",
            "  inflating: stop_sign2.jpg          \n",
            "  inflating: stop_sign3.jpg          \n",
            "  inflating: getimg_ai_img-6WPoZYps7fSXmhggtSkFZ (1).png  \n",
            "  inflating: getimg_ai_img-deNGXpq62npdoz2P75ChF.png  \n",
            "  inflating: getimg_ai_img-mJM6Vor9wKKOIbWobiHM4.png  \n"
          ]
        }
      ],
      "source": [
        "!gdown 1C0k77EeZrByBUdv36IxS9PiLUvZXRr24\n",
        "!unzip -o model.zip\n",
        "\n",
        "!gdown 1Simsmw-H4zKgF5FtJ934bthYulzRlkqz\n",
        "!unzip -o diffusion_imgs.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PhHbmfyY4rTW"
      },
      "outputs": [],
      "source": [
        "with open('params.json', 'r') as config:\n",
        "    params = json.load(config)\n",
        "    class_n_gtsrb = params['GTSRB']['class_n']\n",
        "    device = params['device']\n",
        "    # position_list, _ = load_mask()\n",
        "\n",
        "class GtsrbCNN(nn.Module):\n",
        "    def __init__(self, n_class):\n",
        "\n",
        "        super().__init__()\n",
        "        self.color_map = nn.Conv2d(3, 3, (1, 1), stride=(1, 1), padding=0)\n",
        "        self.module1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, (5, 5), stride=(1, 1), padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, (5, 5), stride=(1, 1), padding=2),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "        )\n",
        "        self.module2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, (5, 5), stride=(1, 1), padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, (5, 5), stride=(1, 1), padding=2),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "        )\n",
        "        self.module3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, (5, 5), stride=(1, 1), padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, (5, 5), stride=(1, 1), padding=2),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "        )\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(14336, 1024, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(1024, 1024, bias=True),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "        )\n",
        "        self.fc3 = nn.Linear(1024, n_class, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.color_map(x)\n",
        "        branch1 = self.module1(x)\n",
        "        branch2 = self.module2(branch1)\n",
        "        branch3 = self.module3(branch2)\n",
        "\n",
        "        branch1 = branch1.reshape(branch1.shape[0], -1)\n",
        "        branch2 = branch2.reshape(branch2.shape[0], -1)\n",
        "        branch3 = branch3.reshape(branch3.shape[0], -1)\n",
        "        concat = torch.cat([branch1, branch2, branch3], 1)\n",
        "\n",
        "        out = self.fc1(concat)\n",
        "        out = self.fc2(out)\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "def pre_process_image(image):\n",
        "\n",
        "    image[:, :, 0] = cv2.equalizeHist(image[:, :, 0])\n",
        "    image[:, :, 1] = cv2.equalizeHist(image[:, :, 1])\n",
        "    image[:, :, 2] = cv2.equalizeHist(image[:, :, 2])\n",
        "    image = image / 255. - .5\n",
        "    return image\n",
        "\n",
        "def transform_image(image, ang_range, shear_range, trans_range, preprocess):\n",
        "\n",
        "    # Rotation\n",
        "    ang_rot = np.random.uniform(ang_range) - ang_range / 2\n",
        "    rows, cols, ch = image.shape\n",
        "    rot_m = cv2.getRotationMatrix2D((cols / 2, rows / 2), ang_rot, 1)\n",
        "\n",
        "    # Translation\n",
        "    tr_x = trans_range * np.random.uniform() - trans_range / 2\n",
        "    tr_y = trans_range * np.random.uniform() - trans_range / 2\n",
        "    trans_m = np.float32([[1, 0, tr_x], [0, 1, tr_y]])\n",
        "\n",
        "    # Shear\n",
        "    pts1 = np.float32([[5, 5], [20, 5], [5, 20]])\n",
        "\n",
        "    pt1 = 5 + shear_range * np.random.uniform() - shear_range / 2\n",
        "    pt2 = 20 + shear_range * np.random.uniform() - shear_range / 2\n",
        "\n",
        "    pts2 = np.float32([[pt1, 5], [pt2, pt1], [5, pt2]])\n",
        "\n",
        "    shear_m = cv2.getAffineTransform(pts1, pts2)\n",
        "\n",
        "    image = cv2.warpAffine(image, rot_m, (cols, rows))\n",
        "    image = cv2.warpAffine(image, trans_m, (cols, rows))\n",
        "    image = cv2.warpAffine(image, shear_m, (cols, rows))\n",
        "\n",
        "    image = pre_process_image(image) if preprocess else image\n",
        "\n",
        "    return image\n",
        "\n",
        "def test_single_image_gtsrb(img_path, label, adv_model=False):\n",
        "\n",
        "    trained_model = GtsrbCNN(n_class=class_n_gtsrb).to(device)\n",
        "    trained_model.load_state_dict(\n",
        "        torch.load(f'{\"adv_\" if adv_model else \"\"}model_gtsrb.pth',\n",
        "                   map_location=torch.device(device)))\n",
        "    trained_model.eval()\n",
        "\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, (32, 32))\n",
        "    img = pre_process_image(img).astype(np.float32)\n",
        "    img = transforms.ToTensor()(img)\n",
        "    img = img.unsqueeze(0).to(device)\n",
        "\n",
        "    predict = torch.softmax(trained_model(img)[0], 0)\n",
        "    index = int(torch.argmax(predict).data)\n",
        "    confidence = float(predict[index].data)\n",
        "\n",
        "    print(f'Correct: {index==label}', end=' ')\n",
        "    print(f'Predict: {index} Confidence: {confidence*100}%')\n",
        "\n",
        "    return index, index == label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4W6fh2485E9b"
      },
      "outputs": [],
      "source": [
        "with open('params.json', 'r') as config:\n",
        "    params = json.load(config)\n",
        "    class_n_lisa = params['LISA']['class_n']\n",
        "    device = params['device']\n",
        "    # position_list, _ = load_mask()\n",
        "\n",
        "\n",
        "class LisaCNN(nn.Module):\n",
        "\n",
        "    def __init__(self, n_class):\n",
        "\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, (8, 8), stride=(2, 2), padding=3)\n",
        "        self.conv2 = nn.Conv2d(64, 128, (6, 6), stride=(2, 2), padding=0)\n",
        "        self.conv3 = nn.Conv2d(128, 128, (5, 5), stride=(1, 1), padding=0)\n",
        "        self.fc = nn.Linear(512, n_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = nn.ReLU()(self.conv1(x))\n",
        "        x = nn.ReLU()(self.conv2(x))\n",
        "        x = nn.ReLU()(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # transforms.Normalize(tuple(mean), tuple(std))\n",
        "])\n",
        "\n",
        "\n",
        "def test_single_image_lisa(img_path, ground_truth, adv_model=False):\n",
        "\n",
        "    trained_model = LisaCNN(n_class=class_n_lisa).to(device)\n",
        "    trained_model.load_state_dict(\n",
        "        torch.load(f'{\"adv_\" if adv_model else \"\"}model_lisa.pth',\n",
        "                   map_location=torch.device(device)))\n",
        "    trained_model.eval()\n",
        "\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, (32, 32))\n",
        "    img = transforms.ToTensor()(img)\n",
        "    img = img.unsqueeze(0).to(device)\n",
        "\n",
        "    predict = torch.softmax(trained_model(img)[0], 0)\n",
        "    index = int(torch.argmax(predict).data)\n",
        "    confidence = float(predict[index].data)\n",
        "\n",
        "    print(f'Correct: {index==ground_truth}', end=' ')\n",
        "    print(f'Predict: {index} Confidence: {confidence*100}%')\n",
        "\n",
        "    return index, index == ground_truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zO4LIHC-5FKr",
        "outputId": "014ccc29-63f3-4352-b10d-4ef194fedf5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./getimg_ai_img-r9pqFJTHiZBSnBJoZBmoz (1).png\n",
            "Correct: False Predict: 12 Confidence: 86.94319128990173%\n",
            "Correct: False Predict: 12 Confidence: 92.0737087726593%\n",
            "Correct: True Predict: 12 Confidence: 23.23242276906967%\n",
            "Correct: True Predict: 12 Confidence: 40.40265679359436%\n",
            "\n",
            "./stop_sign3.jpg\n",
            "Correct: True Predict: 14 Confidence: 28.557923436164856%\n",
            "Correct: True Predict: 14 Confidence: 93.79168152809143%\n",
            "Correct: True Predict: 12 Confidence: 24.735748767852783%\n",
            "Correct: True Predict: 12 Confidence: 27.967163920402527%\n",
            "\n",
            "./getimg_ai_img-DZNGecKlQjJ2KCffiQymf.png\n",
            "Correct: True Predict: 14 Confidence: 90.9130871295929%\n",
            "Correct: False Predict: 13 Confidence: 25.1349538564682%\n",
            "Correct: False Predict: 10 Confidence: 18.08578222990036%\n",
            "Correct: True Predict: 12 Confidence: 58.935362100601196%\n",
            "\n",
            "./getimg_ai_img-uzrz4Pzxm8LipnkHQDHBm.png\n",
            "Correct: True Predict: 14 Confidence: 90.93763828277588%\n",
            "Correct: True Predict: 14 Confidence: 95.77475190162659%\n",
            "Correct: False Predict: 3 Confidence: 17.21704453229904%\n",
            "Correct: True Predict: 12 Confidence: 53.70219945907593%\n",
            "\n",
            "./getimg_ai_img-SiN26Z5uuPC2iKTYWBmua.png\n",
            "Correct: False Predict: 12 Confidence: 45.57012617588043%\n",
            "Correct: False Predict: 13 Confidence: 50.947052240371704%\n",
            "Correct: False Predict: 7 Confidence: 31.764578819274902%\n",
            "Correct: True Predict: 12 Confidence: 41.606929898262024%\n",
            "\n",
            "./road66.png\n",
            "Correct: False Predict: 12 Confidence: 28.48803699016571%\n",
            "Correct: True Predict: 14 Confidence: 89.09096121788025%\n",
            "Correct: True Predict: 12 Confidence: 25.932621955871582%\n",
            "Correct: True Predict: 12 Confidence: 30.610254406929016%\n",
            "\n",
            "./stop_sign2.jpg\n",
            "Correct: False Predict: 12 Confidence: 34.83883738517761%\n",
            "Correct: True Predict: 14 Confidence: 92.57601499557495%\n",
            "Correct: True Predict: 12 Confidence: 25.508731603622437%\n",
            "Correct: True Predict: 12 Confidence: 42.508530616760254%\n",
            "\n",
            "./getimg_ai_img-KYiRhOiO1bePwbp1XkQoT.png\n",
            "Correct: True Predict: 14 Confidence: 90.70943593978882%\n",
            "Correct: True Predict: 14 Confidence: 95.20481824874878%\n",
            "Correct: False Predict: 3 Confidence: 15.649701654911041%\n",
            "Correct: True Predict: 12 Confidence: 56.15890026092529%\n",
            "\n",
            "./stop_sign.jpg\n",
            "Correct: True Predict: 14 Confidence: 33.682143688201904%\n",
            "Correct: True Predict: 14 Confidence: 84.11456942558289%\n",
            "Correct: True Predict: 12 Confidence: 25.34348964691162%\n",
            "Correct: True Predict: 12 Confidence: 20.099911093711853%\n",
            "\n",
            "./road66_sd_16.png\n",
            "Correct: False Predict: 12 Confidence: 34.15997922420502%\n",
            "Correct: True Predict: 14 Confidence: 94.93067264556885%\n",
            "Correct: True Predict: 12 Confidence: 21.184076368808746%\n",
            "Correct: True Predict: 12 Confidence: 23.514315485954285%\n",
            "\n",
            "./getimg_ai_img-6WPoZYps7fSXmhggtSkFZ (1).png\n",
            "Correct: False Predict: 17 Confidence: 29.228928685188293%\n",
            "Correct: False Predict: 25 Confidence: 15.697139501571655%\n",
            "Correct: True Predict: 12 Confidence: 37.332355976104736%\n",
            "Correct: True Predict: 12 Confidence: 39.71858620643616%\n",
            "\n",
            "./getimg_ai_img-deNGXpq62npdoz2P75ChF.png\n",
            "Correct: True Predict: 14 Confidence: 19.7991281747818%\n",
            "Correct: True Predict: 14 Confidence: 58.547937870025635%\n",
            "Correct: True Predict: 12 Confidence: 19.026339054107666%\n",
            "Correct: True Predict: 12 Confidence: 52.537137269973755%\n",
            "\n",
            "./getimg_ai_img-mJM6Vor9wKKOIbWobiHM4.png\n",
            "Correct: False Predict: 12 Confidence: 30.117684602737427%\n",
            "Correct: False Predict: 12 Confidence: 63.2887601852417%\n",
            "Correct: False Predict: 15 Confidence: 39.14968967437744%\n",
            "Correct: True Predict: 12 Confidence: 34.97196435928345%\n",
            "\n",
            "./road66_sd_2.png\n",
            "Correct: True Predict: 14 Confidence: 87.8546953201294%\n",
            "Correct: True Predict: 14 Confidence: 94.27273869514465%\n",
            "Correct: True Predict: 12 Confidence: 31.49103820323944%\n",
            "Correct: True Predict: 12 Confidence: 43.283647298812866%\n",
            "\n",
            "./getimg_ai_img-qbD8yQW1ct0CQgu6BgPCH.png\n",
            "Correct: False Predict: 12 Confidence: 88.60103487968445%\n",
            "Correct: False Predict: 12 Confidence: 35.84955632686615%\n",
            "Correct: False Predict: 7 Confidence: 38.60043287277222%\n",
            "Correct: False Predict: 15 Confidence: 30.238032341003418%\n",
            "\n",
            "./road66_sd_1.png\n",
            "Correct: True Predict: 14 Confidence: 29.82267141342163%\n",
            "Correct: True Predict: 14 Confidence: 92.16907620429993%\n",
            "Correct: True Predict: 12 Confidence: 24.30318593978882%\n",
            "Correct: True Predict: 12 Confidence: 27.997183799743652%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    directory = './'\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.lower().endswith(('.png', '.jpg')):\n",
        "            image_path = os.path.join(directory, filename)\n",
        "            print(image_path)\n",
        "            test_single_image_gtsrb(image_path, 14, adv_model=False)\n",
        "            test_single_image_gtsrb(image_path, 14, adv_model=True)\n",
        "            test_single_image_lisa(image_path, 12, adv_model=False)\n",
        "            test_single_image_lisa(image_path, 12, adv_model=True)\n",
        "            print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}